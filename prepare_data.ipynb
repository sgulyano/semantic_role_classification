{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfec40e7-a3aa-4979-b1f4-271f87729757",
   "metadata": {},
   "source": [
    "# Prepare Data for Semantic Role Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43694dab-11c4-46ca-9768-1c6ca0b37750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import dill\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7616172-2172-49e2-9360-6d761ecb6a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotate_example_fn = 'annotate_example.json'\n",
    "OUTPUT_PATH = './raw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afab60ec-9209-4b0e-bf03-6fcf818458a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3469da8-1999-4138-a8aa-e51ce40e6a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(annotate_example_fn, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca345eaf-76db-4bac-8e3a-ba9dd4ffa40b",
   "metadata": {},
   "source": [
    "Convert a sentence into a list of pairs of word/token and semantic role tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b6cce57-42ce-49e6-a6ad-ccad4562b4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "verb_tag = []\n",
    "\n",
    "for k, v in data.items():\n",
    "    # get color code tag\n",
    "    color_dict = {'#A4A4A4': 'Verb', '': 'Z-O', '#FFFFFF': 'Z-O'}\n",
    "    for r, c, _ in v['summary_concept']:\n",
    "        role = r.split()[0]\n",
    "        color_dict[c] = role\n",
    "    \n",
    "    # convert sentence to array of word and tag pair\n",
    "    for s in v['example_sentence']:\n",
    "        for w, t in s['sent']:\n",
    "            if t not in color_dict:\n",
    "                print(k, s)\n",
    "        \n",
    "        word_list = [(token, color_dict[tag]) for w, tag in s['sent'] for token in w.split('\\xa0') ]\n",
    "        sentences.append(word_list)\n",
    "        verb_tag.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80839fd6-4f9c-43a8-8e88-fe2c98029e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ป้องกัน', 'Z-O'),\n",
       " ('หนู', 'Agent'),\n",
       " ('นา', 'Agent'),\n",
       " ('กัด', 'Verb'),\n",
       " ('กิน', 'Verb'),\n",
       " ('ต้น', 'Object'),\n",
       " ('ข้าว', 'Object')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "215ec14b-2681-4739-aaa9-4460e85e88cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'กัดกิน'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_tag[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c74375e-9259-47e5-9cc7-de575f5f0ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_list = [word[1] for sent in sentences for word in sent]\n",
    "all_ner = sorted(set(ner_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a472cb46-49d7-4607-8e64-ae3ac450f82f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Accompanyment',\n",
       " 'Agent',\n",
       " 'Benefactor',\n",
       " 'Experiencer',\n",
       " 'Instrument',\n",
       " 'Location',\n",
       " 'Manner',\n",
       " 'Measure',\n",
       " 'Object',\n",
       " 'Time',\n",
       " 'Verb',\n",
       " 'Z-O']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450831ca-67d4-4b15-9546-0ff1fb93d1b4",
   "metadata": {},
   "source": [
    "## แบ่งแบบสุ่ม\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "866c145c-7442-419b-b9ba-c59ff7655261",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents, val_sents= train_test_split(sentences, test_size=0.2, random_state=112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8625429-6d19-40ba-a956-a861f5adfe08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1790\n",
      "448\n"
     ]
    }
   ],
   "source": [
    "print(len(train_sents))\n",
    "print(len(val_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fc399dc-3db4-4717-b477-27e46f9ba06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{OUTPUT_PATH}train.data', 'wb') as f:\n",
    "    dill.dump(train_sents, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f11e082-0c05-4fb8-b52e-444d95e17eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{OUTPUT_PATH}val.data', 'wb') as f:\n",
    "    dill.dump(val_sents, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab3bc41-a095-4606-86cd-56d80f1b96d4",
   "metadata": {},
   "source": [
    "## แบ่งแบบ OOV (Out-Of-Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8e8936f-320c-4315-91bc-4e532a6653ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tag, val_tag = train_test_split(np.unique(verb_tag), test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee04048c-4854-499c-9931-6f4c0ab3b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = []\n",
    "val_sents = []\n",
    "for t, s in zip(verb_tag, sentences):\n",
    "    if t in train_tag:\n",
    "        train_sents.append(s)\n",
    "    elif t in val_tag:\n",
    "        val_sents.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6edd868e-ed35-4d57-91f0-743e81720499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1807\n",
      "431\n"
     ]
    }
   ],
   "source": [
    "print(len(train_sents))\n",
    "print(len(val_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "742dc45b-9cc8-499d-8e3a-8c99a3b34c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{OUTPUT_PATH}train_oov.data', 'wb') as f:\n",
    "    dill.dump(train_sents, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74907144-5d19-4fd6-b8d2-61f9e1c7309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{OUTPUT_PATH}val_oov.data', 'wb') as f:\n",
    "    dill.dump(val_sents, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cce4321-3363-47a2-be3c-83cec1532fec",
   "metadata": {},
   "source": [
    "## แบ่งแบบ KFold (K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cfa62ad-199b-4b69-ab16-3ac77cb15ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=5, random_state=112, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=112)\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3868b5c-58a7-43f8-a928-ebd77599ac77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 1790 TEST: 448\n",
      "TRAIN: 1790 TEST: 448\n",
      "TRAIN: 1790 TEST: 448\n",
      "TRAIN: 1791 TEST: 447\n",
      "TRAIN: 1791 TEST: 447\n"
     ]
    }
   ],
   "source": [
    "for k, (train_index, test_index) in enumerate(kf.split(sentences)):\n",
    "    print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "    \n",
    "    train_sents = []\n",
    "    val_sents = []\n",
    "    for i, s in enumerate(sentences):\n",
    "        if i in train_index:\n",
    "            train_sents.append(s)\n",
    "        elif i in test_index:\n",
    "            val_sents.append(s)\n",
    "    \n",
    "    with open(f'{OUTPUT_PATH}train_cv{k}.data', 'wb') as f:\n",
    "        dill.dump(train_sents, f)\n",
    "    \n",
    "    with open(f'{OUTPUT_PATH}val_cv{k}.data', 'wb') as f:\n",
    "        dill.dump(val_sents, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF1",
   "language": "python",
   "name": "tf1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
